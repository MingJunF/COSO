# @package _global_
model:
  name: COSO
  COSO:                              # Missing hyperparameters are to be filled in command line / with tune_hparams = True / selected with +backbone/crn_hparams=...
    _target_: src.models.coso.COSO
    seq_hidden_units:                   # rnn_hidden_units in the original terminology
    br_size:
    fc_hidden_units:
    dropout_rate:                       # Dropout of LSTM hidden layers + output layers
    num_layer: 1
    batch_size:
    alpha: 
    optimizer:
      optimizer_cls: adam
      learning_rate:
      weight_decay: 0.0
      lr_scheduler: False

    tune_hparams: True                # Hparam tuning
    tune_range: 1
    hparams_grid:
      num_layer:
        - 1
      learning_rate:
        - 0.001
      batch_size:
        - 64
        - 128
        - 256
      seq_hidden_units:
        - 0.5
        - 1.0
        - 2.0
        - 3.0
        - 4.0
      br_size:
        - 1.0
      fc_hidden_units:
        - 1.0
      dropout_rate:
        - 0.1
        - 0.2
        - 0.3
        - 0.4
        - 0.5
    resources_per_trial:
      cpu: 0.4
      gpu: 0.2

  encoder:                              # Missing hyperparameters are to be filled in command line / with tune_hparams = True / selected with +backbone/crn_hparams=...
    _target_: src.models.coso.CRNEncoder
    seq_hidden_units:                   # rnn_hidden_units in the original terminology
    br_size:
    fc_hidden_units:
    dropout_rate:                       # Dropout of LSTM hidden layers + output layers
    num_layer: 1
    batch_size:
    optimizer:
      optimizer_cls: adam
      learning_rate:
      weight_decay: 0.0
      lr_scheduler: False

    tune_hparams: False                 # Hparam tuning
    tune_range: 50
    hparams_grid:
    resources_per_trial:

  train_decoder: False
  decoder:                                # Missing hyperparameters are to be filled in command line / with tune_hparams = True / selected with +backbone/crn_hparams=...
    _target_: src.models.coso.CRNDecoder
    seq_hidden_units:                     # rnn_hidden_units in the original terminology
    br_size:
    fc_hidden_units:
    dropout_rate:                         # Dropout of LSTM hidden layers + output layers
    num_layer: 1
    batch_size:
    optimizer:
      optimizer_cls: adam
      learning_rate:
      weight_decay: 0.0
      lr_scheduler: False

    tune_hparams: False                   # Hparam tuning
    tune_range: 30
    hparams_grid:
    resources_per_trial:

exp:
  weights_ema: False
  balancing: grad_reverse